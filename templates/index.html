<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whiskers Chat</title>
    <link rel="stylesheet" href="/static/styles.css">
</head>
<body>
    <div class="container">
        <h1>Chat with Whiskers! ğŸ±</h1>
        <div class="cat-emoji">ğŸ±</div>
        <div id="chatbox" class="chatbox"></div>
        <div id="loading" class="loading" style="display: none;">
            <div class="typing-indicator">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        <div class="input-container">
            <div class="input-wrapper">
                <textarea id="message" placeholder="Type your message here..." maxlength="200" readonly></textarea>
                <button id="mic" class="mic-button" title="Click to start/stop listening">
                    ğŸ¤
                </button>
            </div>
            <div class="buttons">
                <button id="stop" disabled>Stop</button>
                <button id="clear">Clear Chat</button>
            </div>
        </div>
    </div>

    <script>
    const chatbox = document.getElementById('chatbox');
    const messageInput = document.getElementById('message');
    const stopBtn = document.getElementById('stop');
    const clearBtn = document.getElementById('clear');
    const micBtn = document.getElementById('mic');
    const loading = document.getElementById('loading');

    // Initialize speech synthesis
    const synth = window.speechSynthesis;
    let currentUtterance = null;
    let isSpeaking = false;
    let isConversationActive = false;
    let voicesReady = false;

    function loadVoices() {
        if (synth.getVoices().length > 0) {
            voicesReady = true;
            speechSynthesis.removeEventListener('voiceschanged', loadVoices);
        }
    }

    // Attempt to load voices immediately and listen for changes
    loadVoices();
    if (!voicesReady) {
        speechSynthesis.addEventListener('voiceschanged', loadVoices);
    }

    // Check if browser supports speech recognition
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition = null;
    let isListening = false;
    let finalTranscript = '';
    let interimTranscript = '';

    function updateStopButton() {
        stopBtn.disabled = !isConversationActive;
    }

    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            micBtn.classList.add('listening');
            isListening = true;
            isConversationActive = true;
            updateStopButton();
            messageInput.placeholder = "Listening...";
        };

        recognition.onresult = (event) => {
            // Don't process results while Whiskers is speaking
            if (isSpeaking) return;

            interimTranscript = '';
            for (let i = event.resultIndex; i < event.results.length; i++) {
                const transcript = event.results[i][0].transcript;
                if (event.results[i].isFinal) {
                    finalTranscript = transcript;
                    messageInput.value = finalTranscript;
                    // Automatically send the message when final transcript is received
                    sendMessage(finalTranscript);
                } else {
                    interimTranscript += transcript;
                    messageInput.value = interimTranscript;
                }
            }
        };

        recognition.onend = () => {
            if (isListening && !isSpeaking) {
                // Restart recognition if we're still supposed to be listening and not speaking
                try {
                    recognition.start();
                } catch (error) {
                    console.error('Error restarting speech recognition:', error);
                }
            } else {
                micBtn.classList.remove('listening');
                messageInput.placeholder = "Type your message here...";
            }
        };

        recognition.onerror = (event) => {
            console.error('Speech recognition error:', event.error);
            if (event.error === 'no-speech') {
                // Ignore no-speech errors as they're common
                return;
            }
            micBtn.classList.remove('listening');
            isListening = false;
            isConversationActive = false;
            updateStopButton();
            addMessage('Sorry, I had trouble understanding you. Please try again.');
        };
    } else {
        micBtn.style.display = 'none';
        console.warn('Speech recognition not supported in this browser');
    }

    function speak(text) {
        // Filter out emojis and symbols for speech
        const cleanText = text.replace(/[\u{1F300}-\u{1F9FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{1F000}-\u{1F02F}\u{1F0A0}-\u{1F0FF}\u{1F100}-\u{1F64F}\u{1F680}-\u{1F6FF}\u{1F900}-\u{1F9FF}\u{1F1E0}-\u{1F1FF}\u{1F200}-\u{1F2FF}\u{1F600}-\u{1F64F}\u{1F680}-\u{1F6FF}\u{1F700}-\u{1F77F}\u{1F780}-\u{1F7FF}\u{1F800}-\u{1F8FF}\u{1F900}-\u{1F9FF}\u{1FA00}-\u{1FA6F}\u{1FA70}-\u{1FAFF}\u{1FAB0}-\u{1FABF}\u{1FAC0}-\u{1FAFF}\u{1FAD0}-\u{1FAFF}\u{1FAE0}-\u{1FAFF}\u{1FAF0}-\u{1FAFF}\u{1FB00}-\u{1FBFF}\u{1FC00}-\u{1FCFF}\u{1FD00}-\u{1FDFF}\u{1FE00}-\u{1FEFF}\u{1FF00}-\u{1FFFF}\u{2000}-\u{206F}\u{2070}-\u{209F}\u{20A0}-\u{20CF}\u{20D0}-\u{20FF}\u{2100}-\u{214F}\u{2150}-\u{218F}\u{2190}-\u{21FF}\u{2200}-\u{22FF}\u{2300}-\u{23FF}\u{2400}-\u{243F}\u{2440}-\u{245F}\u{2460}-\u{24FF}\u{2500}-\u{257F}\u{2580}-\u{259F}\u{25A0}-\u{25FF}\u{2600}-\u{26FF}\u{2700}-\u{27BF}\u{2800}-\u{28FF}\u{2900}-\u{297F}\u{2980}-\u{29FF}\u{2A00}-\u{2AFF}\u{2B00}-\u{2BFF}\u{2C00}-\u{2C5F}\u{2C60}-\u{2C7F}\u{2C80}-\u{2CFF}\u{2D00}-\u{2D2F}\u{2D30}-\u{2D7F}\u{2D80}-\u{2DDF}\u{2DE0}-\u{2DFF}\u{2E00}-\u{2E7F}\u{2E80}-\u{2EFF}\u{2F00}-\u{2FDF}\u{2FF0}-\u{2FFF}\u{3000}-\u{303F}\u{3040}-\u{309F}\u{30A0}-\u{30FF}\u{3100}-\u{312F}\u{3130}-\u{318F}\u{3190}-\u{319F}\u{31A0}-\u{31BF}\u{31C0}-\u{31EF}\u{31F0}-\u{31FF}\u{3200}-\u{32FF}\u{3300}-\u{33FF}\u{3400}-\u{4DBF}\u{4DC0}-\u{4DFF}\u{4E00}-\u{9FFF}\u{A000}-\u{A48F}\u{A490}-\u{A4CF}\u{A4D0}-\u{A4FF}\u{A500}-\u{A63F}\u{A640}-\u{A69F}\u{A6A0}-\u{A6FF}\u{A700}-\u{A71F}\u{A720}-\u{A7FF}\u{A800}-\u{A82F}\u{A830}-\u{A83F}\u{A840}-\u{A87F}\u{A880}-\u{A8DF}\u{A8E0}-\u{A8FF}\u{A900}-\u{A92F}\u{A930}-\u{A95F}\u{A960}-\u{A97F}\u{A980}-\u{A9DF}\u{A9E0}-\u{A9FF}\u{AA00}-\u{AA5F}\u{AA60}-\u{AA7F}\u{AA80}-\u{AADF}\u{AAE0}-\u{AAFF}\u{AB00}-\u{AB2F}\u{AB30}-\u{AB6F}\u{AB70}-\u{ABBF}\u{ABC0}-\u{ABFF}\u{AC00}-\u{D7AF}\u{D7B0}-\u{D7FF}\u{D800}-\u{DB7F}\u{DB80}-\u{DBFF}\u{DC00}-\u{DFFF}\u{E000}-\u{F8FF}\u{F900}-\u{FAFF}\u{FB00}-\u{FB4F}\u{FB50}-\u{FDFF}\u{FE00}-\u{FE0F}\u{FE10}-\u{FE1F}\u{FE20}-\u{FE2F}\u{FE30}-\u{FE4F}\u{FE50}-\u{FE6F}\u{FE70}-\u{FEFF}\u{FF00}-\u{FFEF}\u{FFF0}-\u{FFFF}]/gu, '');

        if (!voicesReady && synth.getVoices().length === 0) {
            speechSynthesis.addEventListener("voiceschanged", () => speak(text), { once: true });
            synth.getVoices();
            return;
        }
        // Cancel any ongoing speech
        if (currentUtterance) {
            synth.cancel();
        }

        const utterance = new SpeechSynthesisUtterance(cleanText);
        utterance.lang = 'en-US';

        // Configure voice settings for a more natural sound
        utterance.rate = 0.95;
        utterance.pitch = 1.0;
        utterance.volume = 1.0;

        // Try to select a natural-sounding voice if available
        const voices = synth.getVoices();
        const naturalVoice = voices.find(voice =>
            voice.name.includes('Google US English') ||
            voice.name.includes('Google UK English Female') ||
            voice.name.includes('Microsoft Aria') ||
            voice.name.includes('Microsoft Jenny') ||
            voice.name.includes('Samantha')
        );
        if (naturalVoice) {
            utterance.voice = naturalVoice;
        } else {
            const fallbackVoice = voices.find(voice =>
                voice.name.toLowerCase().includes('female') ||
                voice.name.includes('Samantha')
            );
            if (fallbackVoice) {
                utterance.voice = fallbackVoice;
            }
        }

        // Pause recognition while speaking
        if (isListening) {
            recognition.stop();
        }
        isSpeaking = true;
        isConversationActive = true;
        updateStopButton();

        utterance.onstart = () => {
            micBtn.classList.add('speaking');
        };

        utterance.onend = () => {
            isSpeaking = false;
            micBtn.classList.remove('speaking');
            // Resume recognition if we're still supposed to be listening
            if (isListening) {
                try {
                    recognition.start();
                } catch (error) {
                    console.error('Error restarting speech recognition:', error);
                }
            } else {
                isConversationActive = false;
                updateStopButton();
            }
        };

        currentUtterance = utterance;
        synth.speak(utterance);
    }

    function addMessage(text, isUser = false) {
        const div = document.createElement('div');
        div.className = isUser ? 'user' : 'whiskers';
        div.textContent = `${isUser ? 'You' : 'Whiskers'}: ${text}`;
        chatbox.appendChild(div);
        chatbox.scrollTop = chatbox.scrollHeight;

        // Speak out Whiskers' responses
        if (!isUser) {
            speak(text);
        }
    }

    function setLoading(isLoading) {
        loading.style.display = isLoading ? 'block' : 'none';
        isConversationActive = isLoading;
        updateStopButton();
    }

    async function sendMessage(text) {
        if (!text.trim()) return;
        
        addMessage(text, true);
        setLoading(true);
        messageInput.value = '';
        
        try {
            const response = await fetch('/chat', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({message: text})
            });
            const data = await response.json();
            addMessage(data.reply);
        } catch (error) {
            addMessage('Sorry, something went wrong. Please try again.');
        } finally {
            setLoading(false);
        }
    }

    micBtn.addEventListener('click', () => {
        if (!recognition) return;
        
        if (!isListening) {
            try {
                finalTranscript = '';
                interimTranscript = '';
                messageInput.value = '';
                recognition.start();
            } catch (error) {
                console.error('Error starting speech recognition:', error);
            }
        } else {
            isListening = false;
            isConversationActive = false;
            updateStopButton();
            recognition.stop();
        }
    });

    stopBtn.addEventListener('click', () => {
        // Stop speech recognition
        if (isListening) {
            isListening = false;
            recognition.stop();
        }
        
        // Stop any ongoing speech
        if (currentUtterance) {
            synth.cancel();
        }
        
        // Reset states
        isSpeaking = false;
        isConversationActive = false;
        setLoading(false);
        micBtn.classList.remove('listening', 'speaking');
        messageInput.value = '';
        messageInput.placeholder = "Type your message here...";
    });

    clearBtn.addEventListener('click', () => {
        chatbox.innerHTML = '';
        // Stop any ongoing speech
        if (currentUtterance) {
            synth.cancel();
        }
        // Reset states
        isSpeaking = false;
        isConversationActive = false;
        updateStopButton();
    });

    // Add keyboard shortcut for toggling speech recognition
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') {
            if (isListening) {
                isListening = false;
                recognition.stop();
            }
            // Stop any ongoing speech
            if (currentUtterance) {
                synth.cancel();
            }
            // Reset states
            isSpeaking = false;
            isConversationActive = false;
            updateStopButton();
        }
    });

    // Initialize voices when they become available
    if (speechSynthesis.onvoiceschanged !== undefined) {
        speechSynthesis.onvoiceschanged = () => {
            // Voices are now available
            console.log('Voices loaded:', speechSynthesis.getVoices().length);
        };
    }
    </script>
</body>
</html>
